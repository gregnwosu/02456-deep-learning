{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring TensorFlow on GPUs\n",
    "\n",
    "![TensorFlow logo](TensorFlow_logo.jpg)\n",
    "\n",
    "This lab was created by Leo Tam\n",
    "\n",
    "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!  If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later.\n",
    "\n",
    "<iframe id=\"timer\" src=\"timer/timer.html\" width=\"100%\" height=\"120px\"></iframe>\n",
    "\n",
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Shift-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be three: 3\n"
     ]
    }
   ],
   "source": [
    "print \"The answer should be three: \" + str(1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 24 14:43:11 2017       \r\n",
      "+------------------------------------------------------+                       \r\n",
      "| NVIDIA-SMI 346.46     Driver Version: 346.46         |                       \r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           On   | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   29C    P8    17W / 125W |     10MiB /  4095MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GRID K520           On   | 0000:00:04.0     Off |                  N/A |\r\n",
      "| N/A   29C    P8    17W / 125W |     10MiB /  4095MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  GRID K520           On   | 0000:00:05.0     Off |                  N/A |\r\n",
      "| N/A   30C    P8    17W / 125W |     10MiB /  4095MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  GRID K520           On   | 0000:00:06.0     Off |                  N/A |\r\n",
      "| N/A   28C    P8    17W / 125W |     10MiB /  4095MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do something with TensorFlow.  Please run the next two cells, one cell at a time and wait for the cell to complete (the asterisk next to the cell will disappear) prior to running the next cell. You can also tell the system is working by looking at the top right of the page and seeing a solid circle next to \"Python 2\" like ![](files/jupyter_executing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "logdir = '/home/ubuntu'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.platform import gfile\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix1 = tf.placeholder(\"float\",name=\"matrix1\")\n",
    "matrix2 = tf.placeholder(\"float\",name=\"matrix2\")\n",
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have created a matrix multiplication op (matmul) that takes two other ops (tf.constant) as inputs.  Now we must evaluate the graph in a Session object.  Note the data structure is a constant tensor.  Tensors are n-dimensional arrays with a rank, shape, static type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 36.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "result  = sess.run(product,feed_dict={matrix1: [[3., 3.]], matrix2: [[6.],[6.]]})\n",
    "print result\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sessions must be closed to release resources.  We may use the 'with' syntax to close sessions automatically when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 36.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "      result = sess.run(product,feed_dict={matrix1: [[3., 3.]], matrix2: [[6.],[6.]]})\n",
    "      print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have included a device reference, which will determine which GPU to use for operations.  Indexing of devices starts at 0.\n",
    "\n",
    "We may define variables that maintain their properties across executions of the graph.  For instance, a variable is used to track runs of the session.\n",
    "\n",
    "First, create a Variable, that will be initialized to the scalar value 0.  Then, create an Op to add one to `state`.  Variables must be initialized through the use of an 'init' Op after having launched the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "  print sess.run(state)\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print sess.run(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In the following example, we perform simple linear regression.  The target data is $y = 2x + \\eta $ where $ \\eta $ has the distribution ~ $ N(0, \\sigma^2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.   -0.98 -0.96 -0.94 -0.92 -0.9  -0.88 -0.86 -0.84 -0.82 -0.8  -0.78\n",
      " -0.76 -0.74 -0.72 -0.7  -0.68 -0.66 -0.64 -0.62 -0.6  -0.58 -0.56 -0.54\n",
      " -0.52 -0.5  -0.48 -0.46 -0.44 -0.42 -0.4  -0.38 -0.36 -0.34 -0.32 -0.3\n",
      " -0.28 -0.26 -0.24 -0.22 -0.2  -0.18 -0.16 -0.14 -0.12 -0.1  -0.08 -0.06\n",
      " -0.04 -0.02  0.    0.02  0.04  0.06  0.08  0.1   0.12  0.14  0.16  0.18\n",
      "  0.2   0.22  0.24  0.26  0.28  0.3   0.32  0.34  0.36  0.38  0.4   0.42\n",
      "  0.44  0.46  0.48  0.5   0.52  0.54  0.56  0.58  0.6   0.62  0.64  0.66\n",
      "  0.68  0.7   0.72  0.74  0.76  0.78  0.8   0.82  0.84  0.86  0.88  0.9\n",
      "  0.92  0.94  0.96  0.98  1.  ]\n",
      "(101,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc8d3b3da90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc881093810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "x_batch = np.linspace(-1, 1, 101)\n",
    "print x_batch\n",
    "print x_batch.shape\n",
    "y_batch = x_batch * 2 + np.random.randn(*x_batch.shape) * 0.3\n",
    "plt.scatter(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize input Ops using the placeholder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None,), name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=(None,), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a variable for the weights and note that a NumPy array is convertible to a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(np.random.normal(), name=\"W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach here is to perform gradient descent to update a predictor, y_pred, using the least squares cost function.  Updating y_pred is simply done through a matrix multiplication similar to what we have performed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc88c0df1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "y_pred = tf.mul(w, x)\n",
    "y0 = sess.run(y_pred, {x: x_batch})\n",
    "#multiply by x for x in xbatch\n",
    "plt.figure(1)\n",
    "plt.scatter(x_batch, y_batch)\n",
    "plt.plot(x_batch, y0)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(y_pred - y))\n",
    "summary_op = tf.scalar_summary(\"cost\", cost)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial predictor has little relation to the data.\n",
    "\n",
    "We've selected the optimizer to reduce the cost function, which is the sum of squared differences with the data.  We can then define a Summary Writer which will output logs and enable visualizations in TensorBoard. \n",
    "\n",
    "We start our optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.02269\n",
      "6.11042\n",
      "5.31799\n",
      "4.62967\n",
      "4.03178\n",
      "3.51243\n",
      "3.06132\n",
      "2.66947\n",
      "2.3291\n",
      "2.03345\n",
      "1.77664\n",
      "1.55357\n",
      "1.3598\n",
      "1.19149\n",
      "1.04529\n",
      "0.918302\n",
      "0.807995\n",
      "0.712179\n",
      "0.628951\n",
      "0.556658\n",
      "0.493862\n",
      "0.439316\n",
      "0.391936\n",
      "0.35078\n",
      "0.315032\n",
      "0.28398\n",
      "0.257007\n",
      "0.233578\n",
      "0.213227\n",
      "0.195549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc8d3bb3990>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8d219f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_writer = tf.train.SummaryWriter(logdir, sess.graph_def)\n",
    "# if you want this to be part of tensorboard need to define a summary writer\n",
    "for t in range(30):\n",
    "    cost_t, summary, _ = sess.run([cost, summary_op, train_op], {x: x_batch, y: y_batch})\n",
    "    summary_writer.add_summary(summary, t)\n",
    "    print cost_t.mean()\n",
    "\n",
    "y_pred_batch = sess.run(y_pred, {x: x_batch})  \n",
    "plt.figure(1)\n",
    "plt.scatter(x_batch, y_batch)\n",
    "plt.plot(x_batch, y_pred_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.47671088095&quot;).pbtxt = 'node {\\n  name: &quot;matrix1&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;matrix2&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;matrix1&quot;\\n  input: &quot;matrix2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;counter/initial/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;counter&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;counter/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;counter&quot;\\n  input: &quot;counter/initial/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;counter/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;counter&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;counter/read&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;counter&quot;\\n  input: &quot;Add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^counter/Assign&quot;\\n}\\nnode {\\n  name: &quot;x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W/initial/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -2.53935050964\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;W&quot;\\n  input: &quot;W/initial/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init/1&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^counter/Assign&quot;\\n  input: &quot;^W/Assign&quot;\\n}\\nnode {\\n  name: &quot;Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;W/read&quot;\\n  input: &quot;x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Mul&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Rank&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;range/start&quot;\\n  input: &quot;Rank&quot;\\n  input: &quot;range/delta&quot;\\n}\\nnode {\\n  name: &quot;Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;range&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ScalarSummary/tags&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;cost&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ScalarSummary&quot;\\n  op: &quot;ScalarSummary&quot;\\n  input: &quot;ScalarSummary/tags&quot;\\n  input: &quot;Mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Rank&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;range&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Mean/grad/range/start&quot;\\n  input: &quot;gradients/Mean/grad/Rank&quot;\\n  input: &quot;gradients/Mean/grad/range/delta&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Mean/grad/Shape_1&quot;\\n  input: &quot;gradients/Mean/grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Mean/grad/range&quot;\\n  input: &quot;range&quot;\\n  input: &quot;gradients/Mean/grad/Shape&quot;\\n  input: &quot;gradients/Mean/grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/floordiv&quot;\\n  op: &quot;Div&quot;\\n  input: &quot;gradients/Mean/grad/Shape&quot;\\n  input: &quot;gradients/Mean/grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/Mean/grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Mean/grad/Reshape&quot;\\n  input: &quot;gradients/Mean/grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Shape_2&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Shape_3&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Rank_1&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;gradients/Mean/grad/Shape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_1/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_1/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_1&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Mean/grad/range_1/start&quot;\\n  input: &quot;gradients/Mean/grad/Rank_1&quot;\\n  input: &quot;gradients/Mean/grad/range_1/delta&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/Mean/grad/Shape_2&quot;\\n  input: &quot;gradients/Mean/grad/range_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Rank_2&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;gradients/Mean/grad/Shape_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_2/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_2/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_2&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Mean/grad/range_2/start&quot;\\n  input: &quot;gradients/Mean/grad/Rank_2&quot;\\n  input: &quot;gradients/Mean/grad/range_2/delta&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/Mean/grad/Shape_3&quot;\\n  input: &quot;gradients/Mean/grad/range_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/floordiv_1&quot;\\n  op: &quot;Div&quot;\\n  input: &quot;gradients/Mean/grad/Prod&quot;\\n  input: &quot;gradients/Mean/grad/Prod_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/Mean/grad/floordiv_1&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/truediv&quot;\\n  op: &quot;Div&quot;\\n  input: &quot;gradients/Mean/grad/Tile&quot;\\n  input: &quot;gradients/Mean/grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square/grad/mul/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/Mean/grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square/grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Square/grad/mul/x&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square/grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Mean/grad/truediv&quot;\\n  input: &quot;gradients/Square/grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub/grad/Shape&quot;\\n  input: &quot;gradients/sub/grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square/grad/mul_1&quot;\\n  input: &quot;gradients/sub/grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub/grad/Sum&quot;\\n  input: &quot;gradients/sub/grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square/grad/mul_1&quot;\\n  input: &quot;gradients/sub/grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub/grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub/grad/Neg&quot;\\n  input: &quot;gradients/sub/grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub/grad/Reshape&quot;\\n  input: &quot;^gradients/sub/grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub/grad/Reshape&quot;\\n  input: &quot;^gradients/sub/grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub/grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub/grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Mul/grad/Shape&quot;\\n  input: &quot;gradients/Mul/grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/sub/grad/tuple/control_dependency&quot;\\n  input: &quot;x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Mul/grad/mul&quot;\\n  input: &quot;gradients/Mul/grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Mul/grad/Sum&quot;\\n  input: &quot;gradients/Mul/grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;W/read&quot;\\n  input: &quot;gradients/sub/grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Mul/grad/mul_1&quot;\\n  input: &quot;gradients/Mul/grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Mul/grad/Sum_1&quot;\\n  input: &quot;gradients/Mul/grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Mul/grad/Reshape&quot;\\n  input: &quot;^gradients/Mul/grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Mul/grad/Reshape&quot;\\n  input: &quot;^gradients/Mul/grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Mul/grad/Reshape_1&quot;\\n  input: &quot;^gradients/Mul/grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning/rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update/W/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;W&quot;\\n  input: &quot;GradientDescent/learning/rate&quot;\\n  input: &quot;gradients/Mul/grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update/W/ApplyGradientDescent&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.47671088095&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper functions for TF Graph visualization\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "  \n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "tmp_def = rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check you're able to navigate around TensorBoard and navigate to the items below visualizing the graph, weights, and gradient descent parameters.\n",
    "\n",
    "![TensorBoard graph](tensorboard1.png)\n",
    "\n",
    "\n",
    "![TensorBoard weights](tensorboard2.png)\n",
    "\n",
    "\n",
    "![TensorBoard gradient descent](tensorboard3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Convolutional Network\n",
    "\n",
    "In the following section, we use convolutional layers, a crucial tool in networks providing advances over traditional image recognition techniques on large datasets.  Here, we work with a dataset consisting of handwritten integers, the MNIST dataset.  \n",
    "\n",
    "We use a class that stores the MNIST training, validation, and test sets as NumPy arrays.\n",
    "\n",
    "We first initialize the weights and biases.  Weights are typically set to a low noise-like background to avoid 0 gradients providing a small perturbation to the start of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now define a helper function calling the convolution with a stride of one and zero padded to match the input and output size and standard 2x2 max pooling layers.  Under the hood, the TensorFlow functions use the NVIDIA cuDNN (CUDA Deep Neural Network) library to perform assembly optimized implementations on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of our network will be two sets of convolutional and pooling layers, followed by a fully connected layer, regularization layer, and softmax layer. The softmax layer generates probabilities assigned to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above specifies the convolutional and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a Dropout layer, which undersamples the neurons during training to regularize (reduce overfitting) of our model.\n",
    "\n",
    "We now train our model using the similar cross entropy as the objective function and the more robust Adam optimizer. The output is logged for every 100th iteration in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "step 100, training accuracy 0.86\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.82\n",
      "step 400, training accuracy 0.96\n",
      "step 500, training accuracy 0.92\n",
      "step 600, training accuracy 1\n",
      "step 700, training accuracy 0.98\n",
      "step 800, training accuracy 0.88\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.96\n",
      "step 1100, training accuracy 0.94\n",
      "step 1200, training accuracy 1\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 1\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.96\n",
      "step 1700, training accuracy 0.96\n",
      "step 1800, training accuracy 0.94\n",
      "step 1900, training accuracy 0.98\n",
      "step 2000, training accuracy 0.96\n",
      "step 2100, training accuracy 0.96\n",
      "step 2200, training accuracy 1\n",
      "step 2300, training accuracy 1\n",
      "step 2400, training accuracy 0.98\n",
      "step 2500, training accuracy 1\n",
      "step 2600, training accuracy 0.96\n",
      "step 2700, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 2900, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3100, training accuracy 1\n",
      "step 3200, training accuracy 0.98\n",
      "step 3300, training accuracy 1\n",
      "step 3400, training accuracy 0.98\n",
      "step 3500, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3700, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 3900, training accuracy 0.94\n",
      "step 4000, training accuracy 0.98\n",
      "step 4100, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4300, training accuracy 1\n",
      "step 4400, training accuracy 0.98\n",
      "step 4500, training accuracy 0.98\n",
      "step 4600, training accuracy 1\n",
      "step 4700, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 4900, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5100, training accuracy 0.96\n",
      "step 5200, training accuracy 0.96\n",
      "step 5300, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5500, training accuracy 1\n",
      "step 5600, training accuracy 0.98\n",
      "step 5700, training accuracy 0.98\n",
      "step 5800, training accuracy 1\n",
      "step 5900, training accuracy 1\n",
      "test accuracy 0.9885\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(6000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print \"step %d, training accuracy %g\"%(i, train_accuracy)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print \"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.750775373424&quot;).pbtxt = 'node {\\n  name: &quot;matrix1&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;matrix2&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;matrix1&quot;\\n  input: &quot;matrix2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;counter/initial/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;counter&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;counter/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;counter&quot;\\n  input: &quot;counter/initial/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;counter/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;counter&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;counter/read&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;counter&quot;\\n  input: &quot;Add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^counter/Assign&quot;\\n}\\nnode {\\n  name: &quot;x&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W/initial/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -2.53935050964\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;W&quot;\\n  input: &quot;W/initial/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;W/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;W&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init/1&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^counter/Assign&quot;\\n  input: &quot;^W/Assign&quot;\\n}\\nnode {\\n  name: &quot;Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;W/read&quot;\\n  input: &quot;x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Mul&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Rank&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;range/start&quot;\\n  input: &quot;Rank&quot;\\n  input: &quot;range/delta&quot;\\n}\\nnode {\\n  name: &quot;Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;range&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ScalarSummary/tags&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;cost&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ScalarSummary&quot;\\n  op: &quot;ScalarSummary&quot;\\n  input: &quot;ScalarSummary/tags&quot;\\n  input: &quot;Mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Rank&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;range&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Mean/grad/range/start&quot;\\n  input: &quot;gradients/Mean/grad/Rank&quot;\\n  input: &quot;gradients/Mean/grad/range/delta&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Mean/grad/Shape_1&quot;\\n  input: &quot;gradients/Mean/grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Mean/grad/range&quot;\\n  input: &quot;range&quot;\\n  input: &quot;gradients/Mean/grad/Shape&quot;\\n  input: &quot;gradients/Mean/grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/floordiv&quot;\\n  op: &quot;Div&quot;\\n  input: &quot;gradients/Mean/grad/Shape&quot;\\n  input: &quot;gradients/Mean/grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/Mean/grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Mean/grad/Reshape&quot;\\n  input: &quot;gradients/Mean/grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Shape_2&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Square&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Shape_3&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Rank_1&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;gradients/Mean/grad/Shape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_1/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_1/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_1&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Mean/grad/range_1/start&quot;\\n  input: &quot;gradients/Mean/grad/Rank_1&quot;\\n  input: &quot;gradients/Mean/grad/range_1/delta&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/Mean/grad/Shape_2&quot;\\n  input: &quot;gradients/Mean/grad/range_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Rank_2&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;gradients/Mean/grad/Shape_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_2/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_2/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/range_2&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Mean/grad/range_2/start&quot;\\n  input: &quot;gradients/Mean/grad/Rank_2&quot;\\n  input: &quot;gradients/Mean/grad/range_2/delta&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/Mean/grad/Shape_3&quot;\\n  input: &quot;gradients/Mean/grad/range_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/floordiv_1&quot;\\n  op: &quot;Div&quot;\\n  input: &quot;gradients/Mean/grad/Prod&quot;\\n  input: &quot;gradients/Mean/grad/Prod_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/Mean/grad/floordiv_1&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean/grad/truediv&quot;\\n  op: &quot;Div&quot;\\n  input: &quot;gradients/Mean/grad/Tile&quot;\\n  input: &quot;gradients/Mean/grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square/grad/mul/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/Mean/grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square/grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Square/grad/mul/x&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square/grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Mean/grad/truediv&quot;\\n  input: &quot;gradients/Square/grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub/grad/Shape&quot;\\n  input: &quot;gradients/sub/grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square/grad/mul_1&quot;\\n  input: &quot;gradients/sub/grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub/grad/Sum&quot;\\n  input: &quot;gradients/sub/grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square/grad/mul_1&quot;\\n  input: &quot;gradients/sub/grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub/grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub/grad/Neg&quot;\\n  input: &quot;gradients/sub/grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub/grad/Reshape&quot;\\n  input: &quot;^gradients/sub/grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub/grad/Reshape&quot;\\n  input: &quot;^gradients/sub/grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub/grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub/grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub/grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;W/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Mul/grad/Shape&quot;\\n  input: &quot;gradients/Mul/grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/sub/grad/tuple/control_dependency&quot;\\n  input: &quot;x&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Mul/grad/mul&quot;\\n  input: &quot;gradients/Mul/grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Mul/grad/Sum&quot;\\n  input: &quot;gradients/Mul/grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;W/read&quot;\\n  input: &quot;gradients/sub/grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Mul/grad/mul_1&quot;\\n  input: &quot;gradients/Mul/grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Mul/grad/Sum_1&quot;\\n  input: &quot;gradients/Mul/grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Mul/grad/Reshape&quot;\\n  input: &quot;^gradients/Mul/grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Mul/grad/Reshape&quot;\\n  input: &quot;^gradients/Mul/grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mul/grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Mul/grad/Reshape_1&quot;\\n  input: &quot;^gradients/Mul/grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning/rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update/W/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;W&quot;\\n  input: &quot;GradientDescent/learning/rate&quot;\\n  input: &quot;gradients/Mul/grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update/W/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;Placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder/1&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\034\\\\000\\\\000\\\\000\\\\034\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\005\\\\000\\\\000\\\\000\\\\005\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000 \\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;truncated/normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;truncated/normal/TruncatedNormal&quot;\\n  input: &quot;truncated/normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;truncated/normal/mul&quot;\\n  input: &quot;truncated/normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;truncated/normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 32\\n          }\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/1&quot;\\n  input: &quot;Const/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Conv2D&quot;\\n  input: &quot;Variable/1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MaxPool&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;Relu&quot;\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\005\\\\000\\\\000\\\\000\\\\005\\\\000\\\\000\\\\000 \\\\000\\\\000\\\\000@\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_1/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_1/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_1/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;truncated/normal_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_1/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;truncated/normal_1/TruncatedNormal&quot;\\n  input: &quot;truncated/normal_1/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;truncated/normal_1/mul&quot;\\n  input: &quot;truncated/normal_1/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 32\\n        }\\n        dim {\\n          size: 64\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/2&quot;\\n  input: &quot;truncated/normal_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const/2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 64\\n          }\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 64\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/3&quot;\\n  input: &quot;Const/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Conv2D/1&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;MaxPool&quot;\\n  input: &quot;Variable/2/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add/1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Conv2D/1&quot;\\n  input: &quot;Variable/3/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Relu/1&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;add/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MaxPool/1&quot;\\n  op: &quot;MaxPool&quot;\\n  input: &quot;Relu/1&quot;\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;@\\\\014\\\\000\\\\000\\\\000\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_2/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_2/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_2/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;truncated/normal_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_2/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;truncated/normal_2/TruncatedNormal&quot;\\n  input: &quot;truncated/normal_2/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_2&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;truncated/normal_2/mul&quot;\\n  input: &quot;truncated/normal_2/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3136\\n        }\\n        dim {\\n          size: 1024\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/4&quot;\\n  input: &quot;truncated/normal_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const/3&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1024\\n          }\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1024\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/5&quot;\\n  input: &quot;Const/3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377@\\\\014\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;MaxPool/1&quot;\\n  input: &quot;Reshape/1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul/1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape/1&quot;\\n  input: &quot;Variable/4/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add/2&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;MatMul/1&quot;\\n  input: &quot;Variable/5/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Relu/2&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;add/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder/2&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Relu/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/random/uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/random/uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/random/uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;dropout/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/random/uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;dropout/random/uniform/max&quot;\\n  input: &quot;dropout/random/uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/random/uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;dropout/random/uniform/RandomUniform&quot;\\n  input: &quot;dropout/random/uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/random/uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;dropout/random/uniform/mul&quot;\\n  input: &quot;dropout/random/uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Placeholder/2&quot;\\n  input: &quot;dropout/random/uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/Floor&quot;\\n  op: &quot;Floor&quot;\\n  input: &quot;dropout/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/Inv&quot;\\n  op: &quot;Inv&quot;\\n  input: &quot;Placeholder/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Relu/2&quot;\\n  input: &quot;dropout/Inv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dropout/mul/1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;dropout/mul&quot;\\n  input: &quot;dropout/Floor&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\004\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_3/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_3/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_3/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;truncated/normal_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_3/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;truncated/normal_3/TruncatedNormal&quot;\\n  input: &quot;truncated/normal_3/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated/normal_3&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;truncated/normal_3/mul&quot;\\n  input: &quot;truncated/normal_3/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1024\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/6&quot;\\n  input: &quot;truncated/normal_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const/4&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.10000000149\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/7&quot;\\n  input: &quot;Const/4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul/2&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dropout/mul/1&quot;\\n  input: &quot;Variable/6/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add/3&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;MatMul/2&quot;\\n  input: &quot;Variable/7/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;add/3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Log&quot;\\n  op: &quot;Log&quot;\\n  input: &quot;Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Placeholder/1&quot;\\n  input: &quot;Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Rank/1&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/1/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/1/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/1&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;range/1/start&quot;\\n  input: &quot;Rank/1&quot;\\n  input: &quot;range/1/delta&quot;\\n}\\nnode {\\n  name: &quot;Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;mul&quot;\\n  input: &quot;range/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Neg&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/1/Shape&quot;\\n  input: &quot;gradients/1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Neg_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/1/Fill&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/Rank&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;range/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/1/Sum_grad/range/start&quot;\\n  input: &quot;gradients/1/Sum_grad/Rank&quot;\\n  input: &quot;gradients/1/Sum_grad/range/delta&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/1/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/1/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/1/Sum_grad/range&quot;\\n  input: &quot;range/1&quot;\\n  input: &quot;gradients/1/Sum_grad/Shape&quot;\\n  input: &quot;gradients/1/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/floordiv&quot;\\n  op: &quot;Div&quot;\\n  input: &quot;gradients/1/Sum_grad/Shape&quot;\\n  input: &quot;gradients/1/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/Neg_grad/Neg&quot;\\n  input: &quot;gradients/1/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/1/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/1/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Placeholder/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/1/mul_grad/Shape&quot;\\n  input: &quot;gradients/1/mul_grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/1/Sum_grad/Tile&quot;\\n  input: &quot;Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/mul_grad/mul&quot;\\n  input: &quot;gradients/1/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/mul_grad/Sum&quot;\\n  input: &quot;gradients/1/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Placeholder/1&quot;\\n  input: &quot;gradients/1/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/mul_grad/mul_1&quot;\\n  input: &quot;gradients/1/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/1/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/1/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/1/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/1/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Log_grad/Inv&quot;\\n  op: &quot;Inv&quot;\\n  input: &quot;Softmax&quot;\\n  input: &quot;^gradients/1/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Log_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/1/mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/1/Log_grad/Inv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/1/Log_grad/mul&quot;\\n  input: &quot;Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Softmax_grad/mul&quot;\\n  input: &quot;gradients/1/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/1/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/1/Log_grad/mul&quot;\\n  input: &quot;gradients/1/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/1/Softmax_grad/sub&quot;\\n  input: &quot;Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;MatMul/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Variable/7/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/1/add_3_grad/Shape&quot;\\n  input: &quot;gradients/1/add_3_grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/1/add_3_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/add_3_grad/Sum&quot;\\n  input: &quot;gradients/1/add_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/1/add_3_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/add_3_grad/Sum_1&quot;\\n  input: &quot;gradients/1/add_3_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/add_3_grad/Reshape&quot;\\n  input: &quot;^gradients/1/add_3_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/add_3_grad/Reshape&quot;\\n  input: &quot;^gradients/1/add_3_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_3_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/add_3_grad/Reshape_1&quot;\\n  input: &quot;^gradients/1/add_3_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_2_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/1/add_3_grad/tuple/control_dependency&quot;\\n  input: &quot;Variable/6/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_2_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dropout/mul/1&quot;\\n  input: &quot;gradients/1/add_3_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/MatMul_2_grad/MatMul&quot;\\n  input: &quot;^gradients/1/MatMul_2_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/MatMul_2_grad/MatMul&quot;\\n  input: &quot;^gradients/1/MatMul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/MatMul_2_grad/MatMul_1&quot;\\n  input: &quot;^gradients/1/MatMul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dropout/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dropout/Floor&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/1/MatMul_2_grad/tuple/control_dependency&quot;\\n  input: &quot;dropout/Floor&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/mul&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;dropout/mul&quot;\\n  input: &quot;gradients/1/MatMul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/dropout/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/1/dropout/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/1/dropout/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/1/dropout/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Relu/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dropout/Inv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/Shape&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;dropout/Inv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/mul&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/Sum&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Relu/2&quot;\\n  input: &quot;gradients/1/dropout/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/mul_1&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/dropout/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/1/dropout/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/1/dropout/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/dropout/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/1/dropout/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Relu_2_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/1/dropout/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;Relu/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;MatMul/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Variable/5/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/1/add_2_grad/Shape&quot;\\n  input: &quot;gradients/1/add_2_grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Relu_2_grad/ReluGrad&quot;\\n  input: &quot;gradients/1/add_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/add_2_grad/Sum&quot;\\n  input: &quot;gradients/1/add_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Relu_2_grad/ReluGrad&quot;\\n  input: &quot;gradients/1/add_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/add_2_grad/Sum_1&quot;\\n  input: &quot;gradients/1/add_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/add_2_grad/Reshape&quot;\\n  input: &quot;^gradients/1/add_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/add_2_grad/Reshape&quot;\\n  input: &quot;^gradients/1/add_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/add_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/1/add_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_1_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/1/add_2_grad/tuple/control_dependency&quot;\\n  input: &quot;Variable/4/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_1_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Reshape/1&quot;\\n  input: &quot;gradients/1/add_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/MatMul_1_grad/MatMul&quot;\\n  input: &quot;^gradients/1/MatMul_1_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/MatMul_1_grad/MatMul&quot;\\n  input: &quot;^gradients/1/MatMul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MatMul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/MatMul_1_grad/MatMul_1&quot;\\n  input: &quot;^gradients/1/MatMul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;MaxPool/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/MatMul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/1/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MaxPool_1_grad/MaxPoolGrad&quot;\\n  op: &quot;MaxPoolGrad&quot;\\n  input: &quot;Relu/1&quot;\\n  input: &quot;MaxPool/1&quot;\\n  input: &quot;gradients/1/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Relu_1_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/1/MaxPool_1_grad/MaxPoolGrad&quot;\\n  input: &quot;Relu/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Conv2D/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Variable/3/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/1/add_1_grad/Shape&quot;\\n  input: &quot;gradients/1/add_1_grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Relu_1_grad/ReluGrad&quot;\\n  input: &quot;gradients/1/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/add_1_grad/Sum&quot;\\n  input: &quot;gradients/1/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Relu_1_grad/ReluGrad&quot;\\n  input: &quot;gradients/1/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/1/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/1/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/1/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/1/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;MaxPool&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_1_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/1/Conv2D_1_grad/Shape&quot;\\n  input: &quot;Variable/2/read&quot;\\n  input: &quot;gradients/1/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Variable/2/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_1_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;MaxPool&quot;\\n  input: &quot;gradients/1/Conv2D_1_grad/Shape_1&quot;\\n  input: &quot;gradients/1/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/Conv2D_1_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/1/Conv2D_1_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/Conv2D_1_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/1/Conv2D_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/Conv2D_1_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/1/Conv2D_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/MaxPool_grad/MaxPoolGrad&quot;\\n  op: &quot;MaxPoolGrad&quot;\\n  input: &quot;Relu&quot;\\n  input: &quot;MaxPool&quot;\\n  input: &quot;gradients/1/Conv2D_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;ksize&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 2\\n        i: 2\\n        i: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/1/MaxPool_grad/MaxPoolGrad&quot;\\n  input: &quot;Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Variable/1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/1/add_grad/Shape&quot;\\n  input: &quot;gradients/1/add_grad/Shape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/1/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/add_grad/Sum&quot;\\n  input: &quot;gradients/1/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/1/Relu_grad/ReluGrad&quot;\\n  input: &quot;gradients/1/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/1/add_grad/Sum_1&quot;\\n  input: &quot;gradients/1/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/add_grad/Reshape&quot;\\n  input: &quot;^gradients/1/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/add_grad/Reshape&quot;\\n  input: &quot;^gradients/1/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/1/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/1/Conv2D_grad/Shape&quot;\\n  input: &quot;Variable/read&quot;\\n  input: &quot;gradients/1/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;gradients/1/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/1/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/1/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/1/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/1/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/1/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/1/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/1/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta1/power/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.899999976158\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta1/power&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta1/power/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta1/power&quot;\\n  input: &quot;beta1/power/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta1/power/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;beta1/power&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta2/power/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.999000012875\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta2/power&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta2/power/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta2/power&quot;\\n  input: &quot;beta2/power/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta2/power/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;beta2/power&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 5\\n          }\\n          dim {\\n            size: 5\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 32\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/Adam&quot;\\n  input: &quot;zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 5\\n          }\\n          dim {\\n            size: 5\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 32\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam/1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam/1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/Adam/1&quot;\\n  input: &quot;zeros/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam/1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/Adam/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 32\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1/Adam&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/1/Adam&quot;\\n  input: &quot;zeros/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/1/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/3&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 32\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1/Adam_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 32\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/1/Adam_1&quot;\\n  input: &quot;zeros/3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/1/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/1/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/4&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 5\\n          }\\n          dim {\\n            size: 5\\n          }\\n          dim {\\n            size: 32\\n          }\\n          dim {\\n            size: 64\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2/Adam&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 32\\n        }\\n        dim {\\n          size: 64\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/2/Adam&quot;\\n  input: &quot;zeros/4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/2/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/5&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 5\\n          }\\n          dim {\\n            size: 5\\n          }\\n          dim {\\n            size: 32\\n          }\\n          dim {\\n            size: 64\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2/Adam_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 5\\n        }\\n        dim {\\n          size: 32\\n        }\\n        dim {\\n          size: 64\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/2/Adam_1&quot;\\n  input: &quot;zeros/5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/2/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/2/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/6&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 64\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3/Adam&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 64\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/3/Adam&quot;\\n  input: &quot;zeros/6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/3/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/7&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 64\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3/Adam_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 64\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/3/Adam_1&quot;\\n  input: &quot;zeros/7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/3/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/3/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/8&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3136\\n          }\\n          dim {\\n            size: 1024\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4/Adam&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3136\\n        }\\n        dim {\\n          size: 1024\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/4/Adam&quot;\\n  input: &quot;zeros/8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/4/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/9&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3136\\n          }\\n          dim {\\n            size: 1024\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4/Adam_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3136\\n        }\\n        dim {\\n          size: 1024\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/4/Adam_1&quot;\\n  input: &quot;zeros/9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/4/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/4/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/10&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1024\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5/Adam&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1024\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/5/Adam&quot;\\n  input: &quot;zeros/10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/5/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/11&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1024\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5/Adam_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1024\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/5/Adam_1&quot;\\n  input: &quot;zeros/11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/5/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/5/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/12&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1024\\n          }\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6/Adam&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1024\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/6/Adam&quot;\\n  input: &quot;zeros/12&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/6/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/13&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1024\\n          }\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6/Adam_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1024\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/6/Adam_1&quot;\\n  input: &quot;zeros/13&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/6/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/6/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/14&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7/Adam&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/7/Adam&quot;\\n  input: &quot;zeros/14&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/7/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/15&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7/Adam_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/7/Adam_1&quot;\\n  input: &quot;zeros/15&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/7/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/7/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/learning/rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 9.99999974738e-05\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/beta1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.899999976158\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/beta2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.999000012875\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/epsilon&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 9.99999993923e-09\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update/Variable/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;Variable/Adam&quot;\\n  input: &quot;Variable/Adam/1&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/learning/rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/1/Conv2D_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update/Variable_1/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable/1&quot;\\n  input: &quot;Variable/1/Adam&quot;\\n  input: &quot;Variable/1/Adam_1&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/learning/rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/1/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update/Variable_2/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable/2&quot;\\n  input: &quot;Variable/2/Adam&quot;\\n  input: &quot;Variable/2/Adam_1&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/learning/rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/1/Conv2D_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update/Variable_3/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable/3&quot;\\n  input: &quot;Variable/3/Adam&quot;\\n  input: &quot;Variable/3/Adam_1&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/learning/rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/1/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update/Variable_4/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable/4&quot;\\n  input: &quot;Variable/4/Adam&quot;\\n  input: &quot;Variable/4/Adam_1&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/learning/rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/1/MatMul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update/Variable_5/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable/5&quot;\\n  input: &quot;Variable/5/Adam&quot;\\n  input: &quot;Variable/5/Adam_1&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/learning/rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/1/add_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update/Variable_6/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable/6&quot;\\n  input: &quot;Variable/6/Adam&quot;\\n  input: &quot;Variable/6/Adam_1&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/learning/rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/1/MatMul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update/Variable_7/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable/7&quot;\\n  input: &quot;Variable/7/Adam&quot;\\n  input: &quot;Variable/7/Adam_1&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/learning/rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/1/add_3_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;beta1/power/read&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;^Adam/update/Variable/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_1/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_2/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_3/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_4/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_5/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_6/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_7/ApplyAdam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta1/power&quot;\\n  input: &quot;Adam/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/mul/1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;beta2/power/read&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;^Adam/update/Variable/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_1/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_2/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_3/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_4/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_5/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_6/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_7/ApplyAdam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/Assign/1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta2/power&quot;\\n  input: &quot;Adam/mul/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Adam/update/Variable/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_1/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_2/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_3/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_4/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_5/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_6/ApplyAdam&quot;\\n  input: &quot;^Adam/update/Variable_7/ApplyAdam&quot;\\n  input: &quot;^Adam/Assign&quot;\\n  input: &quot;^Adam/Assign/1&quot;\\n}\\nnode {\\n  name: &quot;ArgMax/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;Softmax&quot;\\n  input: &quot;ArgMax/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax/1/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax/1&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;Placeholder/1&quot;\\n  input: &quot;ArgMax/1/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;ArgMax&quot;\\n  input: &quot;ArgMax/1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Rank/2&quot;\\n  op: &quot;Rank&quot;\\n  input: &quot;Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/2/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/2/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;range/2&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;range/2/start&quot;\\n  input: &quot;Rank/2&quot;\\n  input: &quot;range/2/delta&quot;\\n}\\nnode {\\n  name: &quot;Mean/1&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Cast&quot;\\n  input: &quot;range/2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init/2&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^counter/Assign&quot;\\n  input: &quot;^W/Assign&quot;\\n  input: &quot;^Variable/Assign&quot;\\n  input: &quot;^Variable/1/Assign&quot;\\n  input: &quot;^Variable/2/Assign&quot;\\n  input: &quot;^Variable/3/Assign&quot;\\n  input: &quot;^Variable/4/Assign&quot;\\n  input: &quot;^Variable/5/Assign&quot;\\n  input: &quot;^Variable/6/Assign&quot;\\n  input: &quot;^Variable/7/Assign&quot;\\n  input: &quot;^beta1/power/Assign&quot;\\n  input: &quot;^beta2/power/Assign&quot;\\n  input: &quot;^Variable/Adam/Assign&quot;\\n  input: &quot;^Variable/Adam/1/Assign&quot;\\n  input: &quot;^Variable/1/Adam/Assign&quot;\\n  input: &quot;^Variable/1/Adam_1/Assign&quot;\\n  input: &quot;^Variable/2/Adam/Assign&quot;\\n  input: &quot;^Variable/2/Adam_1/Assign&quot;\\n  input: &quot;^Variable/3/Adam/Assign&quot;\\n  input: &quot;^Variable/3/Adam_1/Assign&quot;\\n  input: &quot;^Variable/4/Adam/Assign&quot;\\n  input: &quot;^Variable/4/Adam_1/Assign&quot;\\n  input: &quot;^Variable/5/Adam/Assign&quot;\\n  input: &quot;^Variable/5/Adam_1/Assign&quot;\\n  input: &quot;^Variable/6/Adam/Assign&quot;\\n  input: &quot;^Variable/6/Adam_1/Assign&quot;\\n  input: &quot;^Variable/7/Adam/Assign&quot;\\n  input: &quot;^Variable/7/Adam_1/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.750775373424&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tmp_def = rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer has converged on a reasonable solution with a final test accuracy over 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Autoencoder\n",
    "In the next example, we demonstrate an autoencoder which learns a lower-dimensional representation of sequential input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()\n",
    "ops.reset_default_graph()\n",
    "from tensorflow.models.rnn import rnn_cell, seq2seq\n",
    "sess = tf.InteractiveSession()\n",
    "seq_length = 5\n",
    "batch_size = 64\n",
    "\n",
    "vocab_size = 7\n",
    "embedding_dim = 50\n",
    "\n",
    "memory_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each time point, we define an associated Tensor and label.  Finally, a weights constant is invariant with respect to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_inp = [tf.placeholder(tf.int32, shape=(None,),\n",
    "                          name=\"inp%i\" % t)\n",
    "           for t in range(seq_length)]\n",
    "\n",
    "labels = [tf.placeholder(tf.int32, shape=(None,),\n",
    "                        name=\"labels%i\" % t)\n",
    "          for t in range(seq_length)]\n",
    "\n",
    "weights = [tf.ones_like(labels_t, dtype=tf.float32)\n",
    "           for labels_t in labels]\n",
    "\n",
    "dec_inp = ([tf.zeros_like(enc_inp[0], dtype=np.int32, name=\"GO\")]\n",
    "           + enc_inp[:-1])\n",
    "\n",
    "prev_mem = tf.zeros((batch_size, memory_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a decoder input with the name \"GO\" and dropped the final value of the encoder. We now initialize the seq2seq embedding structure with the previously defined values and apply a loss function that is the cross-entropy across each item in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell = rnn_cell.GRUCell(memory_dim)\n",
    "dec_outputs, dec_memory = seq2seq.embedding_rnn_seq2seq(enc_inp, dec_inp, cell, vocab_size, vocab_size)\n",
    "#creates an rnn that knows that the target for the output is the next character in the sequence, dec_input is the \n",
    "#operation which is initialised to 0's\n",
    "\n",
    "loss = seq2seq.sequence_loss(dec_outputs, labels, weights, vocab_size)\n",
    "#calculate the output loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the outputs during training as the loss and the magnitude of activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.scalar_summary(\"loss\", loss)\n",
    "magnitude = tf.sqrt(tf.reduce_sum(tf.square(dec_outputs[1])))\n",
    "\n",
    "tf.scalar_summary(\"magnitude at t=1\", magnitude)\n",
    "\n",
    "summary_op = tf.merge_all_summaries()\n",
    "\n",
    "logdir = '/home/ubuntu'\n",
    "summary_writer = tf.train.SummaryWriter(logdir, sess.graph_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the learning rate and momentum to our momentum operator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "momentum = 0.9\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we tripled our learning rate and momentum? ([answer](#Answer-#1)).\n",
    "\n",
    "We train in batches on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(batch_size):\n",
    "    X = [np.random.choice(vocab_size, size=(seq_length,), replace=False)\n",
    "         for _ in range(batch_size)]\n",
    "    Y = X[:]\n",
    "    X = np.array(X).T\n",
    "    Y = np.array(Y).T\n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(seq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(seq_length)})\n",
    "    _, loss_t, summary = sess.run([train_op, loss, summary_op], feed_dict)\n",
    "    return loss_t, summary\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for t in range(500):\n",
    "        loss_t, summary = train_batch(batch_size)\n",
    "        summary_writer.add_summary(summary, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our lower dimensional autoencoder by passing data through the embedding to determine if the similar input was recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_batch = [np.random.choice(vocab_size, size=(seq_length,), replace=False)\n",
    "           for _ in range(10)]\n",
    "X_batch = np.array(X_batch).T\n",
    "\n",
    "feed_dict = {enc_inp[t]: X_batch[t] for t in range(seq_length)}\n",
    "dec_outputs_batch = sess.run(dec_outputs, feed_dict)\n",
    "\n",
    "print(X_batch)\n",
    "\n",
    "[logits_t.argmax(axis=1) for logits_t in dec_outputs_batch]\n",
    "\n",
    "tmp_def = rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we may return and implement the changes in learning rate and momemtum to inform us on question 1.\n",
    "\n",
    "\n",
    "# Multi-GPU Computation\n",
    "\n",
    "Now we explore multi-GPU use.  In this exercise, we compute a high degree exponential of two separate matrices and sum them.  The matrix multiplication can be performed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()\n",
    "ops.reset_default_graph()\n",
    "log_device_placement = True\n",
    "n = 10\n",
    "A = np.random.rand(1e4, 1e4).astype('float32')\n",
    "B = np.random.rand(1e4, 1e4).astype('float32')\n",
    "\n",
    "c1 = []\n",
    "c2 = []\n",
    "\n",
    "def matpow(M, n):\n",
    "    if n < 1: #Abstract cases where n < 1\n",
    "        return M\n",
    "    else:\n",
    "        return tf.matmul(M, matpow(M, n-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matpow function simply calls the matrix multiplication repeatedly.  The single GPU computation is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant(A)\n",
    "    b = tf.constant(B)\n",
    "    c1.append(matpow(a, n))\n",
    "    c1.append(matpow(b, n))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  sum = tf.add_n(c1) \n",
    "\n",
    "t1_1 = datetime.datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n",
    "    sess.run(sum)\n",
    "t2_1 = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The add_n function sums all the elements stored in the c1 graph. In this case performing our summation.  The datetime library allows the computation to be timed.\n",
    "\n",
    "Now we perform the multi-GPU computation, computing A^n and B^n on separate GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant(A)\n",
    "    c2.append(matpow(a, n))\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    b = tf.constant(B)\n",
    "    c2.append(matpow(b, n))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  sum = tf.add_n(c2) \n",
    "\n",
    "t1_2 = datetime.datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n",
    "    sess.run(sum)\n",
    "t2_2 = datetime.datetime.now()\n",
    "\n",
    "print \"Single GPU computation time: \" + str(t2_1-t1_1)\n",
    "print \"Multi GPU computation time: \" + str(t2_2-t1_2)\n",
    "tmp_def = rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the effect of performing the summation on a GPU device? ([answer](#Answer-#2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism on GPUs\n",
    "\n",
    "An advanced example is training on the CIFAR-10 image classification training set using multiple GPUS.  CIFAR-10 consists of 32x32 color images in ten categories including airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
    "\n",
    "![cifar](cifar_samples.png)\n",
    "\n",
    "In order to train on multiple GPUs, the model is duplicated in it's own \"tower\", receiving it's own data to be processed on the GPU.  The following section covers updating variables among multiple GPUs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization and import of basic model parameters\n",
    "# pylint: disable=missing-docstring\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import gzip\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "from tensorflow.models.image.cifar10 import cifar10_input\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare constants and hyperparameters. Some constants describe the training process. Op names with the tower name differentiate the operations on GPUs. Note this prefix is removed when visualizing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "IMAGE_SIZE = cifar10_input.IMAGE_SIZE\n",
    "NUM_CLASSES = cifar10_input.NUM_CLASSES\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n",
    "TOWER_NAME = 'tower'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions are used to declare variables in the computation graph, define variables with weight decay, and facilitate data input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "  \"\"\"Helper to create summaries for activations.\n",
    "  Creates a summary that provides a histogram of activations.\n",
    "  Creates a summary that measure the sparsity of activations.\n",
    "  Args:\n",
    "    x: Tensor\n",
    "  Returns:\n",
    "    nothing\n",
    "  \"\"\"\n",
    "  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "  tf.histogram_summary(tensor_name + '/activations', x)\n",
    "  tf.scalar_summary(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    var = tf.get_variable(name, shape, initializer=initializer)\n",
    "  return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  var = _variable_on_cpu(name, shape,\n",
    "                         tf.truncated_normal_initializer(stddev=stddev))\n",
    "  if wd:\n",
    "    weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var\n",
    "\n",
    "def distorted_inputs():\n",
    "  \"\"\"Construct distorted input for CIFAR training using the Reader ops.\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "  Raises:\n",
    "    ValueError: If no data_dir\n",
    "  \"\"\"\n",
    "  if not FLAGS.data_dir:\n",
    "    raise ValueError('Please supply a data_dir')\n",
    "  data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n",
    "  return cifar10_input.distorted_inputs(data_dir=data_dir,\n",
    "                                        batch_size=FLAGS.batch_size)\n",
    "\n",
    "def inputs(eval_data):\n",
    "  \"\"\"Construct input for CIFAR evaluation using the Reader ops.\n",
    "  Args:\n",
    "    eval_data: bool, indicating if one should use the train or eval data set.\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "  Raises:\n",
    "    ValueError: If no data_dir\n",
    "  \"\"\"\n",
    "  if not FLAGS.data_dir:\n",
    "    raise ValueError('Please supply a data_dir')\n",
    "  data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n",
    "  return cifar10_input.inputs(eval_data=eval_data, data_dir=data_dir,\n",
    "                              batch_size=FLAGS.batch_size)\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "  \"\"\"Download and extract the tarball from Alex Krizhevsky's website.\"\"\"\n",
    "  dest_directory = FLAGS.data_dir\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "          float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath,\n",
    "                                             reporthook=_progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define the network architecture.  The architecture consists of two alternating pooling, normalization layers, followed by two locally connected layers and then the softmax activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "  \"\"\"Build the CIFAR-10 model.\n",
    "  Args:\n",
    "    images: Images returned from distorted_inputs() or inputs().\n",
    "  Returns:\n",
    "    Logits.\n",
    "  \"\"\"\n",
    "  # Instantiate all variables using tf.get_variable() instead of\n",
    "  # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "  \n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],\n",
    "                                         stddev=1e-4, wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(bias, name=scope.name)\n",
    "    _activation_summary(conv1)\n",
    "\n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "\n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n",
    "                                         stddev=1e-4, wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(bias, name=scope.name)\n",
    "    _activation_summary(conv2)\n",
    "\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "  # local3\n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    # Move everything into depth so we can perform a single matrix multiply.\n",
    "    dim = 1\n",
    "    for d in pool2.get_shape()[1:].as_list():\n",
    "      dim *= d\n",
    "    reshape = tf.reshape(pool2, [FLAGS.batch_size, dim])\n",
    "\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local3)\n",
    "\n",
    "  # local4\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local4)\n",
    "\n",
    "  # softmax, i.e. softmax(WX + b)\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
    "                                          stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "                              tf.constant_initializer(0.0))\n",
    "    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    _activation_summary(softmax_linear)\n",
    "\n",
    "  return softmax_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function here as the L2 norm of the weights and the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "  \"\"\"Add L2Loss to all the trainable variables.\n",
    "  Add summary for for \"Loss\" and \"Loss/avg\".\n",
    "  Args:\n",
    "    logits: Logits from inference().\n",
    "    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "            of shape [batch_size]\n",
    "  Returns:\n",
    "    Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  # Calculate the average cross entropy loss across the batch.\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits, labels, name='cross_entropy_per_example')\n",
    "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "  tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "  # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "  # decay terms (L2 loss).\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "  \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "  Generates moving average for all losses and associated summaries for\n",
    "  visualizing the performance of the network.\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "  Returns:\n",
    "    loss_averages_op: op for generating moving averages of losses.\n",
    "  \"\"\"\n",
    "  # Compute the moving average of all individual losses and the total loss.\n",
    "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "  losses = tf.get_collection('losses')\n",
    "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "  # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "  # same for the averaged version of the losses.\n",
    "  for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "    tf.scalar_summary(l.op.name +' (raw)', l)\n",
    "    tf.scalar_summary(l.op.name, loss_averages.average(l))\n",
    "\n",
    "  return loss_averages_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set multi-GPU training building on the base model across towers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_gpus', 4,\n",
    "                            \"\"\"How many GPUs to use.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "def tower_loss(scope):\n",
    "  \"\"\"Calculate the total loss on a single tower running the CIFAR model.\n",
    "  Args:\n",
    "    scope: unique prefix string identifying the CIFAR tower, e.g. 'tower_0'\n",
    "  Returns:\n",
    "     Tensor of shape [] containing the total loss for a batch of data\n",
    "  \"\"\"\n",
    "  # Get images and labels for CIFAR-10.\n",
    "  images, labels = distorted_inputs()\n",
    "\n",
    "  # Build inference and loss Graph\n",
    "  logits = inference(images)\n",
    "  _ = loss(logits, labels)\n",
    "\n",
    "  # Assemble all of the losses for the current tower only.\n",
    "  losses = tf.get_collection('losses', scope)\n",
    "\n",
    "  # Calculate the total loss for the current tower.\n",
    "  total_loss = tf.add_n(losses, name='total_loss')\n",
    "\n",
    "  # Compute the moving average of all individual losses and the total loss.\n",
    "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "  # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "  # same for the averaged version of the losses.\n",
    "  for l in losses + [total_loss]:\n",
    "    loss_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', l.op.name)\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "    tf.scalar_summary(loss_name +' (raw)', l)\n",
    "    tf.scalar_summary(loss_name, loss_averages.average(l))\n",
    "\n",
    "  with tf.control_dependencies([loss_averages_op]):\n",
    "    total_loss = tf.identity(total_loss)\n",
    "  return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-GPU approach taken here is through synchronous updates, meaning each model must finishes it's batch and then parameters are averaged across GPUs.\n",
    "\n",
    "We may diagram out our computation graph with the assistance of Tensorboard.  The computation graph is illustrated below.\n",
    "\n",
    "![tensorboardGPUs](modelParallelCifarGPU.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "  \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "  Note that this function provides a synchronization point across all towers.\n",
    "  Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "  Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "  \"\"\"\n",
    "  average_grads = []\n",
    "  for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "    grads = []\n",
    "    for g, _ in grad_and_vars:\n",
    "      # Add 0 dimension to the gradients to represent the tower.\n",
    "      expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "      # Append on a 'tower' dimension which we will average over below.\n",
    "      grads.append(expanded_g)\n",
    "\n",
    "    # Average over the 'tower' dimension.\n",
    "    grad = tf.concat(0, grads)\n",
    "    grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "    # Keep in mind that the Variables are redundant because they are shared\n",
    "    # across towers. So just return the first tower's pointer to the Variable.\n",
    "    v = grad_and_vars[0][1]\n",
    "    grad_and_var = (grad, v)\n",
    "    average_grads.append(grad_and_var)\n",
    "  return average_grads\n",
    "\n",
    "def train():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    # Create a variable to count the number of train() calls. This equals the\n",
    "    # number of batches processed * FLAGS.num_gpus.\n",
    "    global_step = tf.get_variable(\n",
    "        'global_step', [],\n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "    # Calculate the learning rate schedule.\n",
    "    num_batches_per_epoch = (NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /\n",
    "                             FLAGS.batch_size)\n",
    "    decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                    global_step,\n",
    "                                    decay_steps,\n",
    "                                    LEARNING_RATE_DECAY_FACTOR,\n",
    "                                    staircase=True)\n",
    "\n",
    "    # Create an optimizer that performs gradient descent.\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "\n",
    "    # Calculate the gradients for each model tower.\n",
    "    tower_grads = []\n",
    "    for i in xrange(FLAGS.num_gpus):\n",
    "      with tf.device('/gpu:%d' % i):\n",
    "        with tf.name_scope('%s_%d' % (TOWER_NAME, i)) as scope:\n",
    "          # Calculate the loss for one tower of the CIFAR model. This function\n",
    "          # constructs the entire CIFAR model but shares the variables across\n",
    "          # all towers.\n",
    "          loss = tower_loss(scope)\n",
    "\n",
    "          # Reuse variables for the next tower.\n",
    "          tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "          # Retain the summaries from the final tower.\n",
    "          summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "          # Calculate the gradients for the batch of data on this CIFAR tower.\n",
    "          grads = opt.compute_gradients(loss)\n",
    "\n",
    "          # Keep track of the gradients across all towers.\n",
    "          tower_grads.append(grads)\n",
    "\n",
    "    # We must calculate the mean of each gradient. Note that this is the\n",
    "    # synchronization point across all towers.\n",
    "    grads = average_gradients(tower_grads)\n",
    "\n",
    "    # Add a summary to track the learning rate.\n",
    "    summaries.append(tf.scalar_summary('learning_rate', lr))\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "      if grad:\n",
    "        summaries.append(\n",
    "            tf.histogram_summary(var.op.name + '/gradients', grad))\n",
    "\n",
    "    # Apply the gradients to adjust the shared variables.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "      summaries.append(tf.histogram_summary(var.op.name, var))\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # Group all updates to into a single train op.\n",
    "    train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build the summary operation from the last tower summaries.\n",
    "    summary_op = tf.merge_summary(summaries)\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph. allow_soft_placement must be set to\n",
    "    # True to build towers on GPU, as some of the ops do not have GPU\n",
    "    # implementations.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=FLAGS.log_device_placement))\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir,\n",
    "                                            graph_def=sess.graph_def)\n",
    "\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "      _, loss_value = sess.run([train_op, loss])\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "      if step % 10 == 0:\n",
    "        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus\n",
    "        examples_per_sec = num_examples_per_step / duration\n",
    "        sec_per_batch = duration / FLAGS.num_gpus\n",
    "\n",
    "        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                      'sec/batch)')\n",
    "        print (format_str % (datetime.now(), step, loss_value,\n",
    "                             examples_per_sec, sec_per_batch))\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "      # Save the model checkpoint periodically.\n",
    "      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Run the training\n",
    "maybe_download_and_extract()\n",
    "if tf.gfile.Exists(FLAGS.train_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the largest batch size that can be used?\n",
    "\n",
    "# Distributed Training\n",
    "\n",
    "In version 0.7.1 of TensorFlow, a [distributed runtime](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime) was released which allows training to spread in a multi-node fashion.  The distributed runtime uses the Google Remote Procedure Call (gRPC) protocol to manage communication.  The [protocol](http://www.grpc.io/docs/) is an open-source, platform neutral method of communication. \n",
    "\n",
    "We first launch the gRPC server via commands on each node with an example below:\n",
    "\n",
    "bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n",
    "--cluster_spec='worker|192.168.170.193:2500;192.168.170.226:2501' --job_name=worker --task_id=0 &\n",
    "\n",
    "The computation diagram would not appearas:\n",
    "\n",
    "![tensorboardDistributed](modelParallelCifarDistributed.png)\n",
    "\n",
    "We may now generalized multi-GPU example to multi-node as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n",
    "ops.reset_default_graph()\n",
    "n = 4\n",
    "c1 = tf.Variable([])\n",
    "c2 = tf.Variable([])\n",
    "def matpow(M, n):\n",
    "    if n < 1: #Abstract cases where n < 1\n",
    "        return M\n",
    "    else:\n",
    "        return tf.matmul(M, matpow(M, n-1))\n",
    "\n",
    "with tf.device(\"/job:worker/task:0/gpu:0\"):\n",
    "    A = np.random.rand(1e2, 1e2).astype('float32')\n",
    "#     a = tf.constant(A)\n",
    "    c1 = matpow(A,n)\n",
    "    \n",
    "with tf.device(\"/job:worker/task:1/gpu:1\"):\n",
    "    B = np.random.rand(1e2, 1e2).astype('float32')\n",
    "#     b = tf.constant(B)\n",
    "    c2 = matpow(B,n)\n",
    "    \n",
    "t1_3 = datetime.datetime.now()\n",
    "with tf.Session(\"grpc://10.31.115.219:7777\") as sess:\n",
    "    # sess.run(sum)\n",
    "    sum = c1 + c2\n",
    "t2_3 = datetime.datetime.now()\n",
    "print \"Multi node computation time: \" + str(t2_3-t1_3)   \n",
    "\n",
    "tmp_def = rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are executing the matrix multiplies on separate nodes.  \n",
    "\n",
    "The model parallelism approach generalizes well to distributed training.  For the fourth exercise, modify the CIFAR-10 code such that the execution is now model parallel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Significant content borrowed from TensorFlow documentation [here](https://www.tensorflow.org/versions/0.6.0/tutorials/mnist/pros/index.html#build-a-multilayer-convolutional-network), and Aymeric Damien's repository [here](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/multigpu_basics.py). Original model parallel code is licensed under the Apache 2.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommend you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "## More information\n",
    "\n",
    "For more information on using TensorFlow, visit https://www.tensorflow.org/versions/r0.7/tutorials/index.html. A description of the framework, how to use it, and plenty of examples similar to this lesson are posted. \n",
    "\n",
    "To learn more about these other topics, please visit:\n",
    "* GPU accelerated machine learning: [http://www.nvidia.com/object/machine-learning.html](http://www.nvidia.com/object/machine-learning.html)\n",
    "* Theano: [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)\n",
    "* Torch: [http://torch.ch/](http://torch.ch/)\n",
    "* DIGITS: [https://developer.nvidia.com/digits](https://developer.nvidia.com/digits)\n",
    "* cuDNN: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)\n",
    "\n",
    "### Deep Learning Lab Series\n",
    "\n",
    "Make sure to check out the rest of the classes in this Deep Learning lab series.  You can find them [here](https://developer.nvidia.com/deep-learning-courses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer #1\n",
    "\n",
    "What would happen if we tripled our learning rate and momentum? \n",
    "\n",
    "Increasing the learning rate and momentum comes at the risks of skipping local minima.  Here, the learning rate and momentum results in a search that is too coarse and unable to converge.  Increasing momentum reduces the effect of the current time point compared to the previous time points.  This must be balanced with regards to the optimization landscape.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer #2\n",
    "\n",
    "What is the effect of performing the summation on a GPU device?\n",
    "\n",
    "The time is increased marginally.  The extra time stems from transferring data from two GPUs to one GPU (luckily the GPU memory is large enough to fit extra copies or otherwise the operation would fail).  There is a degree of parallelism that is exploited during matrix summations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer #3\n",
    "\n",
    "What is the largest batch size that can be used?\n",
    "\n",
    "The maximum batch size is 8192 for a 12 GB GPU, but diminishing returns are observed after 2048 batch size. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
